{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1680505e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45b4b20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkayleeyvo\u001b[0m (\u001b[33mkayleeyvo-harvard-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c3b759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = f\"dosewise-473716-9f4874e812d6.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d107a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=50, num_layers=2, output_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers, batch_first=True, dropout=0.2\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # Use last time step\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbd56b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=50, num_layers=2, output_size=1):\n",
    "        super(FlexibleLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Single LSTM that can handle both cases\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers, batch_first=True, dropout=0.2\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, use_phen=True):\n",
    "        if not use_phen:\n",
    "            # Mask out PHEN_RATE by setting it to zero\n",
    "            x = x.clone()\n",
    "            x[:, :, 4] = 0  # Set PHEN_RATE (index 4) to zero\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5854bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalPredictor:\n",
    "    def __init__(self, use_wandb=True):\n",
    "        self.model = FlexibleLSTMModel(\n",
    "            input_size=5, hidden_size=50, num_layers=2, output_size=1\n",
    "        )\n",
    "        self.scaler = StandardScaler()\n",
    "        self.sequence_length = 10\n",
    "\n",
    "        # W&B setup\n",
    "        self.use_wandb = use_wandb\n",
    "        if use_wandb:\n",
    "            import wandb\n",
    "\n",
    "            self.wandb = wandb\n",
    "            # Initialize W&B (you'll configure this in your notebook)\n",
    "\n",
    "    def save_model(self, base_path=\"medical_model\"):\n",
    "        \"\"\"Save model, scaler, and metadata locally with timestamp versioning\"\"\"\n",
    "        import os\n",
    "        from datetime import datetime\n",
    "\n",
    "        # Create base directory\n",
    "        os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "        # Add timestamp for versioning\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_filename = f\"model_{timestamp}.pth\"\n",
    "        local_path = f\"{base_path}/{model_filename}\"\n",
    "\n",
    "        # Save model state with timestamp\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": self.model.state_dict(),\n",
    "                \"model_config\": {\n",
    "                    \"input_size\": 5,\n",
    "                    \"hidden_size\": 50,\n",
    "                    \"num_layers\": 2,\n",
    "                    \"output_size\": 1,\n",
    "                },\n",
    "                \"scaler_mean\": (\n",
    "                    self.scaler.mean_ if hasattr(self.scaler, \"mean_\") else None\n",
    "                ),\n",
    "                \"scaler_scale\": (\n",
    "                    self.scaler.scale_ if hasattr(self.scaler, \"scale_\") else None\n",
    "                ),\n",
    "                \"sequence_length\": self.sequence_length,\n",
    "                \"timestamp\": timestamp,  # Include timestamp in model metadata\n",
    "            },\n",
    "            local_path,\n",
    "        )\n",
    "\n",
    "        print(f\"Model saved to {local_path}\")\n",
    "        return local_path  # Return full path for bucket upload\n",
    "\n",
    "    def save_to_bucket(self, bucket_name, local_path):\n",
    "        \"\"\"Upload model files to Google Cloud Storage with timestamp versioning\"\"\"\n",
    "        import os\n",
    "\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(bucket_name)\n",
    "\n",
    "        # Extract filename from local_path\n",
    "        filename = os.path.basename(local_path)\n",
    "\n",
    "        # Upload model file with timestamped name\n",
    "        model_blob = bucket.blob(f\"models/{filename}\")\n",
    "        model_blob.upload_from_filename(local_path)\n",
    "\n",
    "        print(f\"Model uploaded to gs://{bucket_name}/models/{filename}\")\n",
    "\n",
    "    def load_data_from_bigquery(self):\n",
    "        \"\"\"Fetch medical time series data from BigQuery - sampled for speed\"\"\"\n",
    "        client = bigquery.Client()\n",
    "\n",
    "        query = \"\"\"\n",
    "        WITH top_patients AS (\n",
    "        SELECT DISTINCT id\n",
    "        FROM `dosewise-473716.dosewisedb.hemodyn_table`\n",
    "        ORDER BY id\n",
    "        LIMIT 5\n",
    "        )\n",
    "        SELECT \n",
    "            h.id, h.ART, h.ECG_II, h.PLETH, h.CO2, h.PHEN_RATE, h.time\n",
    "        FROM `dosewise-473716.dosewisedb.hemodyn_table` h\n",
    "        INNER JOIN top_patients t ON h.id = t.id\n",
    "        ORDER BY h.id, h.time\n",
    "        \"\"\"\n",
    "\n",
    "        results = client.query(query).result()\n",
    "        df = pd.DataFrame([dict(row) for row in results])\n",
    "\n",
    "        # Debug info\n",
    "        print(f\"Loaded {len(df)} rows from {df['id'].nunique()} patients\")\n",
    "        print(f\"Patient IDs: {sorted(df['id'].unique())}\")\n",
    "        print(f\"Time range: {df['time'].min()} to {df['time'].max()}\")\n",
    "        print(\n",
    "            f\"Features summary:\\n{df[['ART', 'ECG_II', 'PLETH', 'CO2', 'PHEN_RATE']].describe()}\"\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def prepare_sequences(self, df):\n",
    "        \"\"\"Convert time series data to sequences with per-patient scaling\"\"\"\n",
    "        features = [\"ART\", \"ECG_II\", \"PLETH\", \"CO2\", \"PHEN_RATE\"]\n",
    "\n",
    "        all_sequences_X = []\n",
    "        all_sequences_y = []\n",
    "\n",
    "        for patient_id, patient_data in df.groupby(\"id\"):\n",
    "            patient_data = patient_data.sort_values(\"time\")\n",
    "            data = patient_data[features].dropna().values\n",
    "\n",
    "            if len(data) > self.sequence_length:\n",
    "                # Scale PER PATIENT (crucial!)\n",
    "                scaler = StandardScaler()\n",
    "                scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "                X_patient, y_patient = [], []\n",
    "                for i in range(len(scaled_data) - self.sequence_length):\n",
    "                    X_patient.append(scaled_data[i : (i + self.sequence_length)])\n",
    "                    y_patient.append(scaled_data[i + self.sequence_length, 0])\n",
    "\n",
    "                all_sequences_X.extend(X_patient)\n",
    "                all_sequences_y.extend(y_patient)\n",
    "\n",
    "        return np.array(all_sequences_X), np.array(all_sequences_y)\n",
    "\n",
    "    def train_model(self, X, y, epochs=50):\n",
    "        \"\"\"Train the flexible LSTM model with both configurations\"\"\"\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.FloatTensor(y).view(-1, 1)\n",
    "\n",
    "        # W&B: Log training start\n",
    "        if self.use_wandb:\n",
    "            self.wandb.log(\n",
    "                {\"dataset_size\": len(X), \"sequence_length\": self.sequence_length}\n",
    "            )\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get predictions with PHEN_RATE\n",
    "            pred_with_phen = self.model(X_tensor, use_phen=True)\n",
    "            loss_with_phen = criterion(pred_with_phen, y_tensor)\n",
    "\n",
    "            # Get predictions without PHEN_RATE\n",
    "            pred_without_phen = self.model(X_tensor, use_phen=False)\n",
    "            loss_without_phen = criterion(pred_without_phen, y_tensor)\n",
    "\n",
    "            # Total loss (both branches)\n",
    "            total_loss = loss_with_phen + loss_without_phen\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # W&B: Log metrics every epoch\n",
    "            if self.use_wandb and epoch % 5 == 0:  # Log every 5 epochs to avoid spam\n",
    "                self.wandb.log(\n",
    "                    {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"loss_with_phen\": loss_with_phen.item(),\n",
    "                        \"loss_without_phen\": loss_without_phen.item(),\n",
    "                        \"total_loss\": total_loss.item(),\n",
    "                        \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch}/{epochs}], \"\n",
    "                    f\"With PHEN: {loss_with_phen.item():.4f}, \"\n",
    "                    f\"Without PHEN: {loss_without_phen.item():.4f}\"\n",
    "                )\n",
    "\n",
    "    def predict_sequence(self, input_sequence, num_predictions=600):\n",
    "        \"\"\"Generate a sequence of predictions for the next 10 minutes\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Start with the initial input sequence\n",
    "            current_sequence = torch.FloatTensor(input_sequence).unsqueeze(\n",
    "                0\n",
    "            )  # shape: (1, seq_len, 5)\n",
    "            predictions_with_phen = []\n",
    "            predictions_without_phen = []\n",
    "            phen_effects = []\n",
    "\n",
    "            for i in range(num_predictions):\n",
    "                # Get predictions for current sequence\n",
    "                pred_with_phen = self.model(current_sequence, use_phen=True)\n",
    "                pred_without_phen = self.model(current_sequence, use_phen=False)\n",
    "\n",
    "                # Store predictions\n",
    "                pred_with_phen_val = pred_with_phen.item()\n",
    "                pred_without_phen_val = pred_without_phen.item()\n",
    "                phen_effect_val = pred_with_phen_val - pred_without_phen_val\n",
    "\n",
    "                predictions_with_phen.append(pred_with_phen_val)\n",
    "                predictions_without_phen.append(pred_without_phen_val)\n",
    "                phen_effects.append(phen_effect_val)\n",
    "\n",
    "                # Update the sequence for next prediction (autoregressive)\n",
    "                # Remove oldest time step, add new prediction\n",
    "                if i < num_predictions - 1:\n",
    "                    # Create new sequence by shifting window\n",
    "                    new_sequence = current_sequence[\n",
    "                        :, 1:, :\n",
    "                    ].clone()  # Remove oldest time step\n",
    "\n",
    "                    # Create new time step with predicted ART and current features\n",
    "                    last_timestep = current_sequence[\n",
    "                        :, -1:, :\n",
    "                    ].clone()  # Get last timestep\n",
    "\n",
    "                    # Update ART with prediction (with phen version)\n",
    "                    last_timestep[:, :, 0] = pred_with_phen  # ART is index 0\n",
    "\n",
    "                    # Append new timestep\n",
    "                    current_sequence = torch.cat([new_sequence, last_timestep], dim=1)\n",
    "\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    print(f\"Generated {i + 1}/{num_predictions} predictions...\")\n",
    "\n",
    "            if self.use_wandb:\n",
    "                self.wandb.log(\n",
    "                    {\n",
    "                        \"predictions/with_phen_mean\": np.mean(predictions_with_phen),\n",
    "                        \"predictions/with_phen_std\": np.std(predictions_with_phen),\n",
    "                        \"predictions/without_phen_mean\": np.mean(\n",
    "                            predictions_without_phen\n",
    "                        ),\n",
    "                        \"predictions/without_phen_std\": np.std(\n",
    "                            predictions_without_phen\n",
    "                        ),\n",
    "                        \"predictions/phen_effect_mean\": np.mean(phen_effects),\n",
    "                        \"predictions/phen_effect_std\": np.std(phen_effects),\n",
    "                        \"num_predictions\": num_predictions,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Log sample predictions as table\n",
    "                predictions_table = self.wandb.Table(\n",
    "                    columns=[\"Timestep\", \"With_PHEN\", \"Without_PHEN\", \"Effect\"]\n",
    "                )\n",
    "                for i in range(min(50, num_predictions)):  # First 50 predictions\n",
    "                    predictions_table.add_data(\n",
    "                        i + 1,\n",
    "                        predictions_with_phen[i],\n",
    "                        predictions_without_phen[i],\n",
    "                        phen_effects[i],\n",
    "                    )\n",
    "                self.wandb.log({\"sample_predictions\": predictions_table})\n",
    "\n",
    "            return {\n",
    "                \"predictions_with_phen\": predictions_with_phen,\n",
    "                \"predictions_without_phen\": predictions_without_phen,\n",
    "                \"phen_effects\": phen_effects,\n",
    "                \"timestamps\": list(range(1, num_predictions + 1)),\n",
    "            }\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main pipeline\"\"\"\n",
    "        # W&B: Start run (you'll configure this in notebook)\n",
    "        if self.use_wandb:\n",
    "            self.wandb.init(\n",
    "                project=\"dosewise-medical\",\n",
    "                config={\n",
    "                    \"model_type\": \"FlexibleLSTM\",\n",
    "                    \"input_size\": 5,\n",
    "                    \"hidden_size\": 150,\n",
    "                    \"num_layers\": 3,\n",
    "                    \"sequence_length\": 20,\n",
    "                    \"features\": [\"ART\", \"ECG_II\", \"PLETH\", \"CO2\", \"PHEN_RATE\"],\n",
    "                },\n",
    "            )\n",
    "\n",
    "        print(\"Loading data from BigQuery...\")\n",
    "        df = self.load_data_from_bigquery()\n",
    "\n",
    "        # W&B: Log dataset info\n",
    "        if self.use_wandb:\n",
    "            self.wandb.log(\n",
    "                {\n",
    "                    \"data/num_patients\": df[\"id\"].nunique(),\n",
    "                    \"data/num_samples\": len(df),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        print(\"Preparing sequences...\")\n",
    "        X, y = self.prepare_sequences(df)\n",
    "\n",
    "        print(f\"Training flexible model on {len(X)} sequences...\")\n",
    "        self.train_model(X, y, epochs=50)\n",
    "\n",
    "        print(\"Model training completed!\")\n",
    "\n",
    "        # Generate predictions\n",
    "        if len(X) > 0:\n",
    "            print(\"Generating 10-minute prediction sequence...\")\n",
    "            predictions = self.predict_sequence(X[0], num_predictions=600)\n",
    "\n",
    "            # Your existing print statements...\n",
    "\n",
    "        # W&B: Finish run\n",
    "        if self.use_wandb:\n",
    "            self.wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbf55622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20251011_053751-ou995xtq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kayleeyvo-harvard-university/dosewise-medical/runs/ou995xtq' target=\"_blank\">twilight-totem-3</a></strong> to <a href='https://wandb.ai/kayleeyvo-harvard-university/dosewise-medical' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kayleeyvo-harvard-university/dosewise-medical' target=\"_blank\">https://wandb.ai/kayleeyvo-harvard-university/dosewise-medical</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kayleeyvo-harvard-university/dosewise-medical/runs/ou995xtq' target=\"_blank\">https://wandb.ai/kayleeyvo-harvard-university/dosewise-medical/runs/ou995xtq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from BigQuery...\n",
      "Loaded 80637 rows from 5 patients\n",
      "Patient IDs: [np.int64(20), np.int64(28), np.int64(61), np.int64(163), np.int64(185)]\n",
      "Time range: 78414 to 544059\n",
      "Features summary:\n",
      "                ART        ECG_II         PLETH           CO2     PHEN_RATE\n",
      "count  80637.000000  80637.000000  80637.000000  80637.000000  80637.000000\n",
      "mean      74.036060      0.053053     37.622808     21.535812      3.904897\n",
      "std       32.811139      0.305834     14.279384     18.631023      5.307602\n",
      "min     -289.247009     -4.956260     -5.893770      0.000000      0.000000\n",
      "25%       60.312302     -0.028849     32.814499      1.300000      0.000000\n",
      "50%       73.149200      0.020523     37.949299     22.400000      0.100000\n",
      "75%       89.935997      0.089645     44.269001     39.799999      5.100000\n",
      "max      345.687012      5.145430     96.801804     67.800003     30.000000\n",
      "Preparing sequences...\n",
      "Training flexible model on 80587 sequences...\n",
      "Epoch [0/50], With PHEN: 1.0152, Without PHEN: 1.0140\n",
      "Epoch [10/50], With PHEN: 0.9326, Without PHEN: 0.9314\n",
      "Epoch [20/50], With PHEN: 0.7226, Without PHEN: 0.7279\n",
      "Epoch [30/50], With PHEN: 0.5437, Without PHEN: 0.5233\n",
      "Epoch [40/50], With PHEN: 0.4820, Without PHEN: 0.4763\n",
      "Model training completed!\n",
      "Generating 10-minute prediction sequence...\n",
      "Generated 100/600 predictions...\n",
      "Generated 200/600 predictions...\n",
      "Generated 300/600 predictions...\n",
      "Generated 400/600 predictions...\n",
      "Generated 500/600 predictions...\n",
      "Generated 600/600 predictions...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>data/num_patients</td><td>▁</td></tr><tr><td>data/num_samples</td><td>▁</td></tr><tr><td>dataset_size</td><td>▁</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_with_phen</td><td>██▇▆▄▂▂▂▁▁</td></tr><tr><td>loss_without_phen</td><td>██▇▆▄▂▂▂▁▁</td></tr><tr><td>num_predictions</td><td>▁</td></tr><tr><td>predictions/phen_effect_mean</td><td>▁</td></tr><tr><td>predictions/phen_effect_std</td><td>▁</td></tr><tr><td>+6</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>data/num_patients</td><td>5</td></tr><tr><td>data/num_samples</td><td>80637</td></tr><tr><td>dataset_size</td><td>80587</td></tr><tr><td>epoch</td><td>45</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>loss_with_phen</td><td>0.47399</td></tr><tr><td>loss_without_phen</td><td>0.46836</td></tr><tr><td>num_predictions</td><td>600</td></tr><tr><td>predictions/phen_effect_mean</td><td>-0.05514</td></tr><tr><td>predictions/phen_effect_std</td><td>0.00074</td></tr><tr><td>+6</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twilight-totem-3</strong> at: <a href='https://wandb.ai/kayleeyvo-harvard-university/dosewise-medical/runs/ou995xtq' target=\"_blank\">https://wandb.ai/kayleeyvo-harvard-university/dosewise-medical/runs/ou995xtq</a><br> View project at: <a href='https://wandb.ai/kayleeyvo-harvard-university/dosewise-medical' target=\"_blank\">https://wandb.ai/kayleeyvo-harvard-university/dosewise-medical</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code></code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize with W&B\n",
    "predictor = MedicalPredictor(use_wandb=True)\n",
    "\n",
    "# Run the pipeline (W&B will automatically track everything)\n",
    "predictor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a01237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to trained_model/model_20251011_025059.pth\n",
      "Model saved to trained_model/model_20251011_025059.pth\n",
      "Model uploaded to gs://dosewisedb/models/model_20251011_025059.pth\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "model_path = predictor.save_model(\"trained_model\")\n",
    "\n",
    "# Pass the FILE path, not the directory\n",
    "predictor.save_to_bucket(\"dosewisedb\", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68582087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
